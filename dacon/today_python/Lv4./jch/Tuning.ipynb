{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26042cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04623bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scailing\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['fixed acidity']])\n",
    "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
    "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf9767a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(train[['type']])\n",
    "\n",
    "onehot = encoder.transform(train[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "train = pd.concat([train, onehot], axis = 1)\n",
    "train = train.drop(columns = ['type'])\n",
    "\n",
    "onehot = encoder.transform(test[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "test = pd.concat([test, onehot], axis = 1)\n",
    "test = test.drop(columns = ['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e04bbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4dcccc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_bo(max_depth, n_estimators):\n",
    "    rf_params = {\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'n_estimators': int(round(n_estimators))\n",
    "    }\n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, rf.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5d98fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']\n",
    "\n",
    "rf_parameter_bounds = {\n",
    "    'max_depth': (1, 3),\n",
    "    'n_estimators': (30, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e476c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5073  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5218  \u001b[0m | \u001b[95m 2.206   \u001b[0m | \u001b[95m 68.14   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5209  \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 75.21   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5164  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.5445  \u001b[0m | \u001b[95m 2.927   \u001b[0m | \u001b[95m 56.84   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.5627  \u001b[0m | \u001b[95m 2.604   \u001b[0m | \u001b[95m 52.96   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5218  \u001b[0m | \u001b[0m 2.539   \u001b[0m | \u001b[0m 46.04   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.4782  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 53.91   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5318  \u001b[0m | \u001b[0m 2.639   \u001b[0m | \u001b[0m 52.99   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5436  \u001b[0m | \u001b[0m 2.657   \u001b[0m | \u001b[0m 53.03   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds, random_state = 0)\n",
    "BO_rf.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79054276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'n_estimators': 52}\n"
     ]
    }
   ],
   "source": [
    "max_params = BO_rf.max['params']\n",
    "max_params['max_depth'] = int(max_params['max_depth'])\n",
    "max_params['n_estimators'] = int(max_params['n_estimators'])\n",
    "print(max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d23e0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_tuend_rf = RandomForestClassifier(**max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "220d059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization을 이용해 XGBoost 모델 튜닝\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9506e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scailing\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['fixed acidity']])\n",
    "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
    "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac703e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(train[['type']])\n",
    "\n",
    "onehot = encoder.transform(train[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "train = pd.concat([train, onehot], axis = 1)\n",
    "train = train.drop(columns = ['type'])\n",
    "\n",
    "onehot = encoder.transform(test[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "test = pd.concat([test, onehot], axis = 1)\n",
    "test = test.drop(columns = ['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84b9f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def xgb_bo(gamma, max_depth, subsample):\n",
    "    xgb_params = {\n",
    "        'gamma': int(round(gamma)),\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'subsample': int(round(subsample))\n",
    "    }\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    xgb.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a35d416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------\n",
      "[18:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5345  \u001b[0m | \u001b[0m 5.488   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "[18:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.55    \u001b[0m | \u001b[95m 5.449   \u001b[0m | \u001b[95m 1.847   \u001b[0m | \u001b[95m 0.8229  \u001b[0m |\n",
      "[18:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5573  \u001b[0m | \u001b[95m 4.376   \u001b[0m | \u001b[95m 2.784   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "[18:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.5745  \u001b[0m | \u001b[95m 3.834   \u001b[0m | \u001b[95m 2.583   \u001b[0m | \u001b[95m 0.7644  \u001b[0m |\n",
      "[18:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "[18:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5455  \u001b[0m | \u001b[0m 6.692   \u001b[0m | \u001b[0m 2.421   \u001b[0m | \u001b[0m 0.9232  \u001b[0m |\n",
      "[18:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.59    \u001b[0m | \u001b[95m 2.303   \u001b[0m | \u001b[95m 2.915   \u001b[0m | \u001b[95m 0.6916  \u001b[0m |\n",
      "[18:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5482  \u001b[0m | \u001b[0m 5.411   \u001b[0m | \u001b[0m 2.106   \u001b[0m | \u001b[0m 0.8663  \u001b[0m |\n",
      "[18:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5782  \u001b[0m | \u001b[0m 2.303   \u001b[0m | \u001b[0m 2.95    \u001b[0m | \u001b[0m 0.7461  \u001b[0m |\n",
      "[18:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5409  \u001b[0m | \u001b[0m 3.72    \u001b[0m | \u001b[0m 1.231   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']\n",
    "\n",
    "xgb_parameter_bounds = {\n",
    "    'gamma': (0, 10),\n",
    "    'max_depth': (1, 3),\n",
    "    'subsample': (0.5, 1)\n",
    "}\n",
    "\n",
    "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds, random_state = 0)\n",
    "BO_xgb.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4deca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 2.3028884906844747, 'max_depth': 2.915034887195457, 'subsample': 0.6916244375247732}\n"
     ]
    }
   ],
   "source": [
    "print(BO_xgb.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26793054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=2.302, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.6916,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tune = XGBClassifier(gamma = 2.302, max_depth = 2, subsample = 0.6916)\n",
    "xgb_tune.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22835e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_tune.predict(test.drop(columns = ['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1e89b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['quality'] = pred\n",
    "submission.to_csv('tune_xgb.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c47803c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization을 이용해 LightGBM 모델 튜닝\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d073454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scailing\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['fixed acidity']])\n",
    "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
    "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2aedec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(train[['type']])\n",
    "\n",
    "onehot = encoder.transform(train[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "train = pd.concat([train, onehot], axis = 1)\n",
    "train = train.drop(columns = ['type'])\n",
    "\n",
    "onehot = encoder.transform(test[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "test = pd.concat([test, onehot], axis = 1)\n",
    "test = test.drop(columns = ['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07ee1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def lgbm_bo(n_estimators, max_depth, subsample):\n",
    "    lgbm_params = {\n",
    "        'n_estimators': int(round(n_estimators)),\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'subsample': int(round(subsample))\n",
    "    }\n",
    "    lgbm = LGBMClassifier(**lgbm_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0df7b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']\n",
    "\n",
    "lgbm_parameter_bounds = {\n",
    "    'n_estimators': (30, 100),\n",
    "    'max_depth': (1, 3),\n",
    "    'subsample': (0.5, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1f2a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5618  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5691  \u001b[0m | \u001b[95m 2.09    \u001b[0m | \u001b[95m 59.66   \u001b[0m | \u001b[95m 0.8229  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5973  \u001b[0m | \u001b[95m 1.875   \u001b[0m | \u001b[95m 92.42   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5764  \u001b[0m | \u001b[0m 1.767   \u001b[0m | \u001b[0m 85.42   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.56    \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 2.338   \u001b[0m | \u001b[0m 79.72   \u001b[0m | \u001b[0m 0.9232  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5718  \u001b[0m | \u001b[0m 1.977   \u001b[0m | \u001b[0m 91.89   \u001b[0m | \u001b[0m 0.9572  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5345  \u001b[0m | \u001b[0m 2.082   \u001b[0m | \u001b[0m 68.72   \u001b[0m | \u001b[0m 0.8663  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5836  \u001b[0m | \u001b[0m 1.939   \u001b[0m | \u001b[0m 92.29   \u001b[0m | \u001b[0m 0.968   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5373  \u001b[0m | \u001b[0m 1.744   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds, random_state = 0)\n",
    "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99d555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1.875174422525385, 'n_estimators': 92.42411005474558, 'subsample': 0.9818313802505146}\n"
     ]
    }
   ],
   "source": [
    "print(BO_lgbm.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d1f3fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(max_depth=2, n_estimators=92, subsample=0.9818)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_tune = LGBMClassifier(n_estimators = 92, max_depth = 2, subsample = 0.9818)\n",
    "lgbm_tune.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbd08313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgbm_tune.predict(test.drop(columns = ['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "929602f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['quality'] = pred\n",
    "submission.to_csv('tune_lgbm.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
