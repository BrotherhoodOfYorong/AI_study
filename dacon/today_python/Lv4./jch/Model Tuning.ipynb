{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0be6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# RandomForest 튜닝\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fe7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scailing\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['fixed acidity']])\n",
    "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
    "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323fe0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(train[['type']])\n",
    "\n",
    "onehot = encoder.transform(train[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "train = pd.concat([train, onehot], axis = 1)\n",
    "train = train.drop(columns = ['type'])\n",
    "\n",
    "onehot = encoder.transform(test[['type']])\n",
    "onehot = onehot.toarray()\n",
    "onehot = pd.DataFrame(onehot)\n",
    "onehot.columns = encoder.get_feature_names()\n",
    "test = pd.concat([test, onehot], axis = 1)\n",
    "test = test.drop(columns = ['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd867b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b903a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameter_bounds = {\n",
    "    'max_depth': (1, 3),\n",
    "    'n_estimators': (30, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1271b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_bo(max_depth, n_estimators):\n",
    "    rf_params = {\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'n_estimators': int(round(n_estimators))\n",
    "    }\n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, rf.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa17ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5164  \u001b[0m | \u001b[0m 2.206   \u001b[0m | \u001b[0m 68.14   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5127  \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 75.21   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5191  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5155  \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 56.84   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4518  \u001b[0m | \u001b[0m 1.018   \u001b[0m | \u001b[0m 48.01   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.54    \u001b[0m | \u001b[0m 1.963   \u001b[0m | \u001b[0m 82.94   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.54    \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 30.0    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.4436  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 35.23   \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.5527  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 86.77   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)\n",
    "BO_rf.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37d3dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 튜닝\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b7163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_parameter_bounds = {\n",
    "    'gamma': (0, 10),\n",
    "    'max_depth': (1, 3),\n",
    "    'subsample': (0.5, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fef0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_bo(gamma, max_depth, subsample):\n",
    "    xgb_params = {\n",
    "        'gamma': int(round(gamma)),\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'subsample': int(round(subsample))\n",
    "    }\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    xgb.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb24f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------\n",
      "[18:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 5.488   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "[18:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5482  \u001b[0m | \u001b[95m 5.449   \u001b[0m | \u001b[95m 1.847   \u001b[0m | \u001b[95m 0.8229  \u001b[0m |\n",
      "[18:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5882  \u001b[0m | \u001b[95m 4.376   \u001b[0m | \u001b[95m 2.784   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "[18:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5745  \u001b[0m | \u001b[0m 3.834   \u001b[0m | \u001b[0m 2.583   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
      "[18:44:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.57    \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "[18:44:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5518  \u001b[0m | \u001b[0m 4.387   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.5544  \u001b[0m |\n",
      "[18:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5782  \u001b[0m | \u001b[0m 2.303   \u001b[0m | \u001b[0m 2.915   \u001b[0m | \u001b[0m 0.6916  \u001b[0m |\n",
      "[18:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5527  \u001b[0m | \u001b[0m 5.411   \u001b[0m | \u001b[0m 2.106   \u001b[0m | \u001b[0m 0.8663  \u001b[0m |\n",
      "[18:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 4.196   \u001b[0m | \u001b[0m 2.621   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[18:44:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5536  \u001b[0m | \u001b[0m 3.72    \u001b[0m | \u001b[0m 1.231   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
    "BO_xgb.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30b5edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 튜닝\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = train.drop(columns = ['index', 'quality'])\n",
    "y = train['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d7d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_parameter_bounds = {\n",
    "    'n_estimators': (30, 100),\n",
    "    'max_depth': (1, 3),\n",
    "    'subsample': (0.5, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63012426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_bo(n_estimators, max_depth, subsample):\n",
    "    lgbm_params = {\n",
    "        'n_estimators': int(round(n_estimators)),\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'subsample': int(round(subsample))\n",
    "    }\n",
    "    lgbm = LGBMClassifier(**lgbm_params)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b98e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5655  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.57    \u001b[0m | \u001b[95m 2.09    \u001b[0m | \u001b[95m 59.66   \u001b[0m | \u001b[95m 0.8229  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5736  \u001b[0m | \u001b[95m 1.875   \u001b[0m | \u001b[95m 92.42   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.5773  \u001b[0m | \u001b[95m 1.767   \u001b[0m | \u001b[95m 85.42   \u001b[0m | \u001b[95m 0.7644  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5564  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5391  \u001b[0m | \u001b[0m 1.829   \u001b[0m | \u001b[0m 85.39   \u001b[0m | \u001b[0m 0.8606  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5418  \u001b[0m | \u001b[0m 1.461   \u001b[0m | \u001b[0m 97.03   \u001b[0m | \u001b[0m 0.6916  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5473  \u001b[0m | \u001b[0m 2.082   \u001b[0m | \u001b[0m 68.72   \u001b[0m | \u001b[0m 0.8663  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 1.391   \u001b[0m | \u001b[0m 49.28   \u001b[0m | \u001b[0m 0.5432  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 1.744   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.9286  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
    "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f4a43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3.0, 'n_estimators': 86.77395944328902}\n",
      "{'gamma': 4.375872112626925, 'max_depth': 2.7835460015641598, 'subsample': 0.9818313802505146}\n",
      "{'max_depth': 1.7668830376515554, 'n_estimators': 85.42075266578652, 'subsample': 0.7644474598764522}\n"
     ]
    }
   ],
   "source": [
    "print(BO_rf.max['params'])\n",
    "print(BO_xgb.max['params'])\n",
    "print(BO_lgbm.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d4386c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM = LGBMClassifier(max_depth = 2, n_estimators = 85, subsample = 0.7644)\n",
    "XGB = XGBClassifier(gamma = 4.375, max_depth = 3, subsample = 0.9818)\n",
    "RF = RandomForestClassifier(max_depth = 3, n_estimators = 87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee825360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d8313c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one = pd.get_dummies(train)\n",
    "test_one = pd.get_dummies(test)\n",
    "\n",
    "X = train_one.drop('quality', axis = 1)\n",
    "y = train_one['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba73a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freshman\\Desktop\\AI\\ai\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:52:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(max_depth=3,\n",
       "                                                     n_estimators=87)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, gamma=4.375,\n",
       "                                            gpu_id=None, importance_type='gain',\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=3,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=0.9818, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None)),\n",
       "                             ('lgbm',\n",
       "                              LGBMClassifier(max_depth=2, n_estimators=85,\n",
       "                                             subsample=0.7644))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eba6fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = VC.predict(test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea23a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['quality'] = pred\n",
    "submission.to_csv('tune_voting.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473be0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
